{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.19.0.dev20240508)\n",
      "Requirement already satisfied: torch in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.4.0.dev20240508)\n",
      "Requirement already satisfied: einops in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: omegaconf in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.66.2)\n",
      "Requirement already satisfied: pytorch-lightning in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.2.4)\n",
      "Requirement already satisfied: torchmetrics==0.11.4 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torchmetrics==0.11.4->-r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torchmetrics==0.11.4->-r requirements.txt (line 7)) (24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 1)) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from omegaconf->-r requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from omegaconf->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.11.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->-r requirements.txt (line 6)) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/mihirpotnis/miniforge3/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihirpotnis/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LatentDiffusion.forward() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldsrlib/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your config.yaml file\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Upscale the image\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mupscale_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mupscale_image\u001b[0;34m(input_image_path, output_image_path, model_path, config_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Run the upscaling\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m     upscaled_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Convert tensor back to image\u001b[39;00m\n\u001b[1;32m     33\u001b[0m upscaled_image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray((upscaled_tensor\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LatentDiffusion.forward() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from omegaconf import OmegaConf\n",
    "from os import path\n",
    "\n",
    "def upscale_image(input_image_path, output_image_path, model_path, config_path):\n",
    "    # Load and normalize the input image\n",
    "    image = PIL.Image.open(input_image_path)\n",
    "    ldsr = LDSR(modelPath=model_path, torchdevice=torch.device(\"mps\"))\n",
    "    normalized_image, w_pad, h_pad = LDSR.normalize_image(image)\n",
    "\n",
    "    # Convert image to tensor\n",
    "    image_tensor = torch.tensor(np.array(normalized_image)).float().div(255).permute(2, 0, 1).unsqueeze(0).to(ldsr.torchdevice)\n",
    "\n",
    "    # Load the model from checkpoint\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(model_path, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = ldsr.load_model_from_config()\n",
    "\n",
    "    model['model'].load_state_dict(sd, strict=False)\n",
    "    model['model'].to(ldsr.torchdevice)\n",
    "    model['model'].eval()\n",
    "\n",
    "    # Run the upscaling\n",
    "    with torch.no_grad():\n",
    "        upscaled_tensor = model['model'](image_tensor)\n",
    "\n",
    "    # Convert tensor back to image\n",
    "    upscaled_image = PIL.Image.fromarray((upscaled_tensor.squeeze(0).permute(1, 2, 0).clamp(0, 1).mul(255).byte().cpu().numpy()))\n",
    "\n",
    "    # Remove padding\n",
    "    upscaled_image = LDSR.remove_padding(image, upscaled_image, w_pad, h_pad)\n",
    "\n",
    "    # Save the upscaled image\n",
    "    upscaled_image.save(output_image_path)\n",
    "    print(f\"Upscaled image saved to {output_image_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"example_lowres.png\"\n",
    "    output_image_path = \"image_upscaled.png\"\n",
    "    model_path = \"last.ckpt\"  # Path to your local .ckpt file\n",
    "    config_path = \"ldsrlib/config.yaml\"  # Path to your config.yaml file\n",
    "\n",
    "    # Upscale the image\n",
    "    upscale_image(input_image_path, output_image_path, model_path, config_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 576 but got size 144 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldsrlib/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your config.yaml file\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Upscale the image\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mupscale_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mupscale_image\u001b[0;34m(input_image_path, output_image_path, model_path, config_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run the upscaling\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 37\u001b[0m     upscaled_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditioning_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Convert tensor back to image\u001b[39;00m\n\u001b[1;32m     40\u001b[0m upscaled_image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray((upscaled_tensor\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:879\u001b[0m, in \u001b[0;36mLatentDiffusion.forward\u001b[0;34m(self, x, c, *args, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m         tc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_ids[t]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    878\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start\u001b[38;5;241m=\u001b[39mc, t\u001b[38;5;241m=\u001b[39mtc, noise\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn_like(c\u001b[38;5;241m.\u001b[39mfloat()))\n\u001b[0;32m--> 879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:1015\u001b[0m, in \u001b[0;36mLatentDiffusion.p_losses\u001b[0;34m(self, x_start, cond, t, noise)\u001b[0m\n\u001b[1;32m   1013\u001b[0m noise \u001b[38;5;241m=\u001b[39m default(noise, \u001b[38;5;28;01mlambda\u001b[39;00m: torch\u001b[38;5;241m.\u001b[39mrandn_like(x_start))\n\u001b[1;32m   1014\u001b[0m x_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start\u001b[38;5;241m=\u001b[39mx_start, t\u001b[38;5;241m=\u001b[39mt, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[0;32m-> 1015\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1018\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:987\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[1;32m    984\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m fold(o) \u001b[38;5;241m/\u001b[39m normalization\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_recon, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_ids:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:1406\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model(x, t)\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1406\u001b[0m     xc \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model(xc, t)\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 576 but got size 144 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from omegaconf import OmegaConf\n",
    "from os import path\n",
    "\n",
    "def upscale_image(input_image_path, output_image_path, model_path, config_path):\n",
    "    # Detect the device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load and normalize the input image\n",
    "    image = PIL.Image.open(input_image_path)\n",
    "    ldsr = LDSR(modelPath=model_path, torchdevice=device)\n",
    "    normalized_image, w_pad, h_pad = LDSR.normalize_image(image)\n",
    "\n",
    "    # Prepare the conditioning input (e.g., a low-resolution version of the input image)\n",
    "    conditioning_image = normalized_image.resize((normalized_image.width // 4, normalized_image.height // 4))\n",
    "    conditioning_tensor = torch.tensor(np.array(conditioning_image)).float().div(255).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert image to tensor\n",
    "    image_tensor = torch.tensor(np.array(normalized_image)).float().div(255).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load the model from checkpoint\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(model_path, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = ldsr.load_model_from_config()\n",
    "\n",
    "    model['model'].load_state_dict(sd, strict=False)\n",
    "    model['model'].to(device)\n",
    "    model['model'].eval()\n",
    "\n",
    "    # Run the upscaling\n",
    "    with torch.no_grad():\n",
    "        upscaled_tensor = model['model'](image_tensor, c=conditioning_tensor)\n",
    "\n",
    "    # Convert tensor back to image\n",
    "    upscaled_image = PIL.Image.fromarray((upscaled_tensor.squeeze(0).permute(1, 2, 0).clamp(0, 1).mul(255).byte().cpu().numpy()))\n",
    "\n",
    "    # Remove padding\n",
    "    upscaled_image = LDSR.remove_padding(image, upscaled_image, w_pad, h_pad)\n",
    "\n",
    "    # Save the upscaled image\n",
    "    upscaled_image.save(output_image_path)\n",
    "    print(f\"Upscaled image saved to {output_image_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"example_lowres.png\"\n",
    "    output_image_path = \"image_upscaled.png\"\n",
    "    model_path = \"last.ckpt\"  # Path to your local .ckpt file\n",
    "    config_path = \"ldsrlib/config.yaml\"  # Path to your config.yaml file\n",
    "\n",
    "    # Upscale the image\n",
    "    upscale_image(input_image_path, output_image_path, model_path, config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 576 but got size 144 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldsrlib/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your config.yaml file\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Upscale the image\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mupscale_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mupscale_image\u001b[0;34m(input_image_path, output_image_path, model_path, config_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run the upscaling\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 41\u001b[0m     upscaled_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditioning_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Convert tensor back to image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m upscaled_image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray((upscaled_tensor\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:879\u001b[0m, in \u001b[0;36mLatentDiffusion.forward\u001b[0;34m(self, x, c, *args, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m         tc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_ids[t]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    878\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start\u001b[38;5;241m=\u001b[39mc, t\u001b[38;5;241m=\u001b[39mtc, noise\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn_like(c\u001b[38;5;241m.\u001b[39mfloat()))\n\u001b[0;32m--> 879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:1015\u001b[0m, in \u001b[0;36mLatentDiffusion.p_losses\u001b[0;34m(self, x_start, cond, t, noise)\u001b[0m\n\u001b[1;32m   1013\u001b[0m noise \u001b[38;5;241m=\u001b[39m default(noise, \u001b[38;5;28;01mlambda\u001b[39;00m: torch\u001b[38;5;241m.\u001b[39mrandn_like(x_start))\n\u001b[1;32m   1014\u001b[0m x_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start\u001b[38;5;241m=\u001b[39mx_start, t\u001b[38;5;241m=\u001b[39mt, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[0;32m-> 1015\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1018\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:987\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[1;32m    984\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m fold(o) \u001b[38;5;241m/\u001b[39m normalization\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_recon, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_ids:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:1406\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model(x, t)\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1406\u001b[0m     xc \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model(xc, t)\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 576 but got size 144 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from omegaconf import OmegaConf\n",
    "from os import path\n",
    "\n",
    "def upscale_image(input_image_path, output_image_path, model_path, config_path):\n",
    "    # Detect the device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load and normalize the input image\n",
    "    image = PIL.Image.open(input_image_path)\n",
    "    ldsr = LDSR(modelPath=model_path, torchdevice=device)\n",
    "    normalized_image, w_pad, h_pad = LDSR.normalize_image(image)\n",
    "\n",
    "    # Prepare the conditioning input (e.g., a low-resolution version of the input image)\n",
    "    # Assuming the model is designed for 4x upscaling, resize the conditioning input to 1/4th of the normalized image\n",
    "    conditioning_image = normalized_image.resize(\n",
    "        (normalized_image.width // 4, normalized_image.height // 4), \n",
    "        PIL.Image.LANCZOS\n",
    "    )\n",
    "    conditioning_tensor = torch.tensor(np.array(conditioning_image)).float().div(255).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert image to tensor\n",
    "    image_tensor = torch.tensor(np.array(normalized_image)).float().div(255).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load the model from checkpoint\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(model_path, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = ldsr.load_model_from_config()\n",
    "\n",
    "    model['model'].load_state_dict(sd, strict=False)\n",
    "    model['model'].to(device)\n",
    "    model['model'].eval()\n",
    "\n",
    "    # Run the upscaling\n",
    "    with torch.no_grad():\n",
    "        upscaled_tensor = model['model'](image_tensor, c=conditioning_tensor)\n",
    "\n",
    "    # Convert tensor back to image\n",
    "    upscaled_image = PIL.Image.fromarray((upscaled_tensor.squeeze(0).permute(1, 2, 0).clamp(0, 1).mul(255).byte().cpu().numpy()))\n",
    "\n",
    "    # Remove padding\n",
    "    upscaled_image = LDSR.remove_padding(image, upscaled_image, w_pad, h_pad)\n",
    "\n",
    "    # Save the upscaled image\n",
    "    upscaled_image.save(output_image_path)\n",
    "    print(f\"Upscaled image saved to {output_image_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"example_lowres.png\"\n",
    "    output_image_path = \"image_upscaled.png\"\n",
    "    model_path = \"last.ckpt\"  # Path to your local .ckpt file\n",
    "    config_path = \"ldsrlib/config.yaml\"  # Path to your config.yaml file\n",
    "\n",
    "    # Upscale the image\n",
    "    upscale_image(input_image_path, output_image_path, model_path, config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m custom_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Example number of steps, adjust based on your needs\u001b[39;00m\n\u001b[1;32m     41\u001b[0m eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Example eta value, adjust based on your needs\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m ldsr_output \u001b[38;5;241m=\u001b[39m \u001b[43mldsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[1;32m     45\u001b[0m output_image \u001b[38;5;241m=\u001b[39m ldsr_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/LDSR.py:219\u001b[0m, in \u001b[0;36mLDSR.run\u001b[0;34m(self, model, image, task, custom_steps, eta, resize_enabled, classifier_ckpt, global_step)\u001b[0m\n\u001b[1;32m    216\u001b[0m make_progrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    217\u001b[0m custom_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m height, width \u001b[38;5;241m=\u001b[39m \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    220\u001b[0m split_input \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m width \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_input:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Assuming the necessary classes and functions are defined in LDSR.py\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from ldsrlib.ldm.util import instantiate_from_config\n",
    "\n",
    "# Path to the model checkpoint\n",
    "model_ckpt = 'last.ckpt'\n",
    "config_file = 'ldsrlib/config.yaml'  # Ensure this is the correct path to the config file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Initialize the model\n",
    "ldsr = LDSR(modelPath=model_ckpt, yamlPath=config_file)\n",
    "\n",
    " # Load the model from checkpoint\n",
    "config = OmegaConf.load(config_file)\n",
    "pl_sd = torch.load(model_ckpt, map_location=\"cpu\")\n",
    "sd = pl_sd[\"state_dict\"]\n",
    "model = ldsr.load_model_from_config()\n",
    "\n",
    "model['model'].load_state_dict(sd, strict=False)\n",
    "model['model'].to(device)\n",
    "model['model'].eval()\n",
    "\n",
    "# Load and prepare the input image\n",
    "input_image_path = \"example_lowres.png\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Normalize and prepare the image for processing\n",
    "input_image, w_pad, h_pad = LDSR.normalize_image(input_image)\n",
    "\n",
    "# Convert image to tensor\n",
    "input_tensor = torch.tensor(np.array(input_image)).float().div(255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Perform upscaling\n",
    "task = 'super_resolution'  # Example task, replace with the actual task if different\n",
    "custom_steps = 5  # Example number of steps, adjust based on your needs\n",
    "eta = 0.5  # Example eta value, adjust based on your needs\n",
    "ldsr_output = ldsr.run(model, input_tensor, task, custom_steps, eta)\n",
    "\n",
    "# Post-process the output\n",
    "output_image = ldsr_output[\"sample\"].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "output_image = (output_image * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert numpy array back to image\n",
    "output_image_pil = Image.fromarray(output_image)\n",
    "\n",
    "# Save or display the output image\n",
    "output_image_pil.save(\"image_upscaled.png\")\n",
    "output_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ldsr_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Example eta value, adjust based on your needs\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#ldsr_output = ldsr.run(model, example, task, custom_steps, eta)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output_image \u001b[38;5;241m=\u001b[39m \u001b[43mldsr_output\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     54\u001b[0m output_image \u001b[38;5;241m=\u001b[39m (output_image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Convert numpy array back to image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ldsr_output' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Assuming the necessary classes and functions are defined in LDSR.py\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from ldsrlib.ldm.util import instantiate_from_config\n",
    "\n",
    "# Path to the model checkpoint\n",
    "model_ckpt = 'last.ckpt'\n",
    "config_file = 'ldsrlib/config.yaml'  # Ensure this is the correct path to the config file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "ldsr = LDSR(modelPath=model_ckpt, yamlPath=config_file)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "config = OmegaConf.load(config_file)\n",
    "pl_sd = torch.load(model_ckpt, map_location=\"cpu\")\n",
    "sd = pl_sd[\"state_dict\"]\n",
    "model = ldsr.load_model_from_config()\n",
    "\n",
    "model['model'].load_state_dict(sd, strict=False)\n",
    "model['model'].to(device)\n",
    "model['model'].eval()\n",
    "\n",
    "# Load and prepare the input image\n",
    "input_image_path = \"example_lowres.png\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Normalize and prepare the image for processing\n",
    "input_image, w_pad, h_pad = LDSR.normalize_image(input_image)\n",
    "\n",
    "# Convert image to tensor\n",
    "input_tensor = torch.tensor(np.array(input_image)).float().div(255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Constructing the input dictionary as expected by the LDSR model\n",
    "example = {\n",
    "    \"image\": input_tensor.to(device),\n",
    "    # Include additional keys if required by the model\n",
    "}\n",
    "\n",
    "# Perform upscaling\n",
    "task = 'super_resolution'  # Example task, replace with the actual task if different\n",
    "custom_steps = 50  # Example number of steps, adjust based on your needs\n",
    "eta = 0.5  # Example eta value, adjust based on your needs\n",
    "\n",
    "#ldsr_output = ldsr.run(model, example, task, custom_steps, eta)\n",
    "\n",
    "# Post-process the output\n",
    "output_image = ldsr_output[\"sample\"].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "output_image = (output_image * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert numpy array back to image\n",
    "output_image_pil = Image.fromarray(output_image)\n",
    "\n",
    "# Save or display the output image\n",
    "output_image_pil.save(\"image_upscaled.png\")\n",
    "output_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihirpotnis/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "LDSR\n",
      "ddpm\n",
      "Plotting: Switched to EMA weights\n",
      "Sampling with eta = 1.0; steps: 100\n",
      "Data shape for DDIM sampling is (1, 3, 576, 576), eta 1.0\n",
      "Running DDIM Sampling with 100 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:   0%|          | 0/100 [00:00<?, ?it/s]ERROR:tornado.general:SEND Error: Host unreachable\n",
      "DDIM Sampler:   1%|          | 1/100 [02:56<4:51:39, 176.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outofpsample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:   2%|         | 2/100 [03:00<2:02:27, 74.97s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outofpsample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:   2%|         | 2/100 [03:01<2:27:57, 90.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting: Restored training weights\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m upscaler \u001b[38;5;241m=\u001b[39m \u001b[38;5;21m__init__\u001b[39m\u001b[38;5;241m.\u001b[39mLDSRUpscaler()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Call the upscale method\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m ldsr_output \u001b[38;5;241m=\u001b[39m \u001b[43mupscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_downscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_downscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[1;32m    110\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255.\u001b[39m \u001b[38;5;241m*\u001b[39m ldsr_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/__init__.py:111\u001b[0m, in \u001b[0;36mLDSRUpscaler.upscale\u001b[0;34m(self, model, images, steps, pre_downscale, post_downscale, downsample_method)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    110\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mldsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuperResolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_downscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_downscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample_method\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mstack(outputs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/LDSR.py:301\u001b[0m, in \u001b[0;36mLDSR.superResolution\u001b[0;34m(self, image, ddimSteps, preDownScale, postDownScale, downsampleMethod)\u001b[0m\n\u001b[1;32m    297\u001b[0m     im_og \u001b[38;5;241m=\u001b[39m im_og\u001b[38;5;241m.\u001b[39mresize((width_downsampled_pre, height_downsampled_pre), Image\u001b[38;5;241m.\u001b[39mLANCZOS)\n\u001b[1;32m    299\u001b[0m im_og, w_pad, h_pad \u001b[38;5;241m=\u001b[39m LDSR\u001b[38;5;241m.\u001b[39mnormalize_image(im_og)\n\u001b[0;32m--> 301\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim_og\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffMode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m sample \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    304\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/LDSR.py:246\u001b[0m, in \u001b[0;36mLDSR.run\u001b[0;34m(self, model, image, task, custom_steps, eta, resize_enabled, classifier_ckpt, global_step)\u001b[0m\n\u001b[1;32m    243\u001b[0m         x_T \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, custom_shape[\u001b[38;5;241m1\u001b[39m], custom_shape[\u001b[38;5;241m2\u001b[39m], custom_shape[\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m         x_T \u001b[38;5;241m=\u001b[39m repeat(x_T, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 c h w -> b c h w\u001b[39m\u001b[38;5;124m'\u001b[39m, b\u001b[38;5;241m=\u001b[39mcustom_shape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 246\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mmake_convolutional_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43minvert_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcustom_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mresize_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_enabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcorrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43msave_intermediate_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_intermediate_vid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmake_progrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_progrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddim_use_x0_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_x0_pred\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/LDSR.py:141\u001b[0m, in \u001b[0;36mLDSR.run.<locals>.make_convolutional_sample\u001b[0;34m(batch, model, mode, custom_steps, eta, swap_mode, masked, invert_mask, quantize_x0, custom_schedule, decode_interval, resize_enabled, custom_shape, temperature, noise_dropout, corrector, corrector_kwargs, x_T, save_intermediate_vid, make_progrow, ddim_use_x0_pred)\u001b[0m\n\u001b[1;32m    139\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    140\u001b[0m img_cb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m sample, intermediates \u001b[38;5;241m=\u001b[39m \u001b[43mconvsample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mquantize_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ddim_use_x0_pred:\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/LDSR.py:176\u001b[0m, in \u001b[0;36mLDSR.run.<locals>.convsample_ddim\u001b[0;34m(model, cond, steps, shape, eta, callback, normals_sequence, mask, x0, quantize_x0, img_callback, temperature, noise_dropout, score_corrector, corrector_kwargs, x_T, log_every_t)\u001b[0m\n\u001b[1;32m    174\u001b[0m shape \u001b[38;5;241m=\u001b[39m shape[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# cut batch dim\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampling with eta = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[43mddim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnormals_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormals_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddim.py:94\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m size \u001b[38;5;241m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData shape for DDIM sampling is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddim.py:147\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[1;32m    144\u001b[0m     img_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mq_sample(x0, ts)  \u001b[38;5;66;03m# TODO: deterministic forward pass?\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m img_orig \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m*\u001b[39m img\n\u001b[0;32m--> 147\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m img, pred_x0 \u001b[38;5;241m=\u001b[39m outs\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutofpsample\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddim.py:171\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[1;32m    168\u001b[0m b, \u001b[38;5;241m*\u001b[39m_, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unconditional_conditioning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m unconditional_guidance_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     x_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:977\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[1;32m    974\u001b[0m     cond_list \u001b[38;5;241m=\u001b[39m [cond \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]  \u001b[38;5;66;03m# Todo make this more efficient\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# apply model by loop over crops\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m output_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(z_list[i], t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcond_list[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    979\u001b[0m                       \u001b[38;5;28mtuple\u001b[39m)  \u001b[38;5;66;03m# todo cant deal with multiple model outputs check this never happens\u001b[39;00m\n\u001b[1;32m    981\u001b[0m o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(output_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:977\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    974\u001b[0m     cond_list \u001b[38;5;241m=\u001b[39m [cond \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]  \u001b[38;5;66;03m# Todo make this more efficient\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# apply model by loop over crops\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m output_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    979\u001b[0m                       \u001b[38;5;28mtuple\u001b[39m)  \u001b[38;5;66;03m# todo cant deal with multiple model outputs check this never happens\u001b[39;00m\n\u001b[1;32m    981\u001b[0m o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(output_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/models/diffusion/ddpm.py:1410\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1409\u001b[0m     xc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m+\u001b[39m c_concat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1410\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1412\u001b[0m     cc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(c_crossattn, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/modules/diffusionmodules/openaimodel.py:732\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[0;32m--> 732\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    734\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(h, emb, context)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/modules/diffusionmodules/openaimodel.py:83\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, TimestepBlock):\n\u001b[0;32m---> 83\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[1;32m     85\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, context)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/modules/diffusionmodules/openaimodel.py:250\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, emb):\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Apply the block to a Tensor, conditioned on a timestep embedding.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    :param x: an [N x C x ...] Tensor of features.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    :param emb: an [N x emb_channels] Tensor of timestep embeddings.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    :return: an [N x C x ...] Tensor of outputs.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_checkpoint\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/modules/diffusionmodules/util.py:114\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointFunction\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;28mlen\u001b[39m(inputs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/ldm/modules/diffusionmodules/openaimodel.py:274\u001b[0m, in \u001b[0;36mResBlock._forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m emb_out\n\u001b[0;32m--> 274\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_layers\u001b[49m(h)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connection(x) \u001b[38;5;241m+\u001b[39m h\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1697\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import __init__\n",
    "\n",
    "# Assuming necessary imports from ldsrlib and related modules\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from ldsrlib.ldm.util import instantiate_from_config\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageSequence\n",
    "import torch\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Ensure the image_path is correct\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"The specified image path does not exist: {image_path}\")\n",
    "\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    output_images = []\n",
    "    output_masks = []\n",
    "    w, h = None, None\n",
    "\n",
    "    excluded_formats = ['MPO']\n",
    "    \n",
    "    for i in ImageSequence.Iterator(img):\n",
    "        # Transpose the image based on EXIF data\n",
    "        i = ImageOps.exif_transpose(i)\n",
    "\n",
    "        if i.mode == 'I':\n",
    "            i = i.point(lambda i: i * (1 / 255))\n",
    "        image = i.convert(\"RGB\")\n",
    "\n",
    "        if len(output_images) == 0:\n",
    "            w = image.size[0]\n",
    "            h = image.size[1]\n",
    "        \n",
    "        if image.size[0] != w or image.size[1] != h:\n",
    "            continue\n",
    "        \n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).unsqueeze(0)\n",
    "        if 'A' in i.getbands():\n",
    "            mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n",
    "            mask = 1. - torch.from_numpy(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((64, 64), dtype=torch.float32)\n",
    "        output_images.append(image)\n",
    "        output_masks.append(mask.unsqueeze(0))\n",
    "\n",
    "    if len(output_images) > 1 and img.format not in excluded_formats:\n",
    "        output_image = torch.cat(output_images, dim=0)\n",
    "        output_mask = torch.cat(output_masks, dim=0)\n",
    "    else:\n",
    "        output_image = output_images[0]\n",
    "        output_mask = output_masks[0]\n",
    "\n",
    "    return output_image, output_mask\n",
    "\n",
    "# Example usage\n",
    "image_path = 'example_lowres.png'  # Update this to the correct image path\n",
    "output_image, output_mask = load_image(image_path)\n",
    "\n",
    "\n",
    "# Path to the model checkpoint and configuration file\n",
    "model_ckpt = 'last.ckpt'\n",
    "config_file = 'ldsrlib/config.yaml'  # Ensure this is the correct path to the config file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "ldsr = LDSR(modelPath=model_ckpt, yamlPath=config_file)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "config = OmegaConf.load(config_file)\n",
    "pl_sd = torch.load(model_ckpt, map_location=\"cpu\")\n",
    "sd = pl_sd[\"state_dict\"]\n",
    "model = ldsr.load_model_from_config()\n",
    "\n",
    "model['model'].load_state_dict(sd, strict=False)\n",
    "model['model'].to(device)\n",
    "model['model'].eval()\n",
    "\n",
    "# Load and prepare the input image\n",
    "input_image_path = \"example_lowres.png\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Normalize and prepare the image for processing\n",
    "#input_image, w_pad, h_pad = LDSR.normalize_image(input_image)\n",
    "\n",
    "# Convert image to tensor\n",
    "input_tensor = torch.tensor(np.array(input_image)).float().div(255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Call the upscale function\n",
    "steps = \"100\"  # Example number of steps, adjust based on your needs\n",
    "pre_downscale = \"None\"\n",
    "post_downscale = \"None\"\n",
    "downsample_method = \"Lanczos\"\n",
    "\n",
    "# Initialize LDSRUpscale\n",
    "upscaler = __init__.LDSRUpscaler()\n",
    "\n",
    "# Call the upscale method\n",
    "ldsr_output = upscaler.upscale('last.ckpt', output_image, steps, pre_downscale, post_downscale, downsample_method)\n",
    "\n",
    "# Post-process the output\n",
    "i = 255. * ldsr_output[0].cpu().numpy()\n",
    "img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n",
    "#output_image = ldsr_output[0].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "#output_image = (output_image * 255).clip(0, 255).astype(np.uint8)\n",
    "print('out of func')\n",
    "# Convert numpy array back to image\n",
    "#output_image_pil = Image.fromarray(output_image)\n",
    "\n",
    "# Save or display the output image\n",
    "img.save(\"image_upscaled.png\")\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LDSRUpscale' from 'ldsrlib' (/Users/mihirpotnis/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/ComfyUI-Flowty-LDSR-master/ComfyUI-Flowty-LDSR-master\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldsrlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLDSR\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LDSR\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldsrlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LDSRUpscale\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Path to the model checkpoint and configuration file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LDSRUpscale' from 'ldsrlib' (/Users/mihirpotnis/Downloads/ComfyUI-Flowty-LDSR-master/ldsrlib/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add the path to the ldsrlib to the system path\n",
    "sys.path.append('/mnt/data/ComfyUI-Flowty-LDSR-master/ComfyUI-Flowty-LDSR-master')\n",
    "\n",
    "from ldsrlib.LDSR import LDSR\n",
    "from ldsrlib import LDSRUpscale\n",
    "\n",
    "# Path to the model checkpoint and configuration file\n",
    "model_ckpt = 'last.ckpt'\n",
    "config_file = 'ldsrlib/config.yaml'  # Ensure this is the correct path to the config file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "ldsr = LDSR(modelPath=model_ckpt, yamlPath=config_file)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "config = OmegaConf.load(config_file)\n",
    "pl_sd = torch.load(model_ckpt, map_location=\"cpu\")\n",
    "sd = pl_sd[\"state_dict\"]\n",
    "model = ldsr.load_model_from_config()\n",
    "\n",
    "model['model'].load_state_dict(sd, strict=False)\n",
    "model['model'].to(device)\n",
    "model['model'].eval()\n",
    "\n",
    "# Load and prepare the input image\n",
    "input_image_path = \"example_lowres.png\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Normalize and prepare the image for processing\n",
    "input_image, w_pad, h_pad = LDSR.normalize_image(input_image)\n",
    "\n",
    "# Convert image to tensor\n",
    "input_tensor = torch.tensor(np.array(input_image)).float().div(255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Call the upscale function\n",
    "steps = \"100\"  # Example number of steps, adjust based on your needs\n",
    "pre_downscale = \"None\"\n",
    "post_downscale = \"None\"\n",
    "downsample_method = \"Lanczos\"\n",
    "\n",
    "# Initialize LDSRUpscale\n",
    "upscaler = LDSRUpscale()\n",
    "\n",
    "# Call the upscale method\n",
    "ldsr_output = upscaler.upscale(model, [input_tensor.to(device)], steps, pre_downscale, post_downscale, downsample_method)\n",
    "\n",
    "# Post-process the output\n",
    "output_image = ldsr_output[0].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "output_image = (output_image * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert numpy array back to image\n",
    "output_image_pil = Image.fromarray(output_image)\n",
    "\n",
    "# Save or display the output image\n",
    "output_image_pil.save(\"image_upscaled.png\")\n",
    "output_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ldsrlib' has no attribute 'LDSRUpscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m downsample_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanczos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize LDSRUpscale through the imported module\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m upscaler \u001b[38;5;241m=\u001b[39m \u001b[43mldsrlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLDSRUpscale\u001b[49m()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Call the upscale method\u001b[39;00m\n\u001b[1;32m     52\u001b[0m ldsr_output \u001b[38;5;241m=\u001b[39m upscaler\u001b[38;5;241m.\u001b[39mupscale(model, [input_tensor\u001b[38;5;241m.\u001b[39mto(device)], steps, pre_downscale, post_downscale, downsample_method)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ldsrlib' has no attribute 'LDSRUpscale'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add the path to the ldsrlib to the system path\n",
    "sys.path.append('/mnt/data/ComfyUI-Flowty-LDSR-master/ComfyUI-Flowty-LDSR-master')\n",
    "\n",
    "from ldsrlib.LDSR import LDSR\n",
    "import ldsrlib  # Import the entire module to access LDSRUpscale through __init__.py\n",
    "\n",
    "# Path to the model checkpoint and configuration file\n",
    "model_ckpt = 'last.ckpt'\n",
    "config_file = 'ldsrlib/config.yaml'  # Ensure this is the correct path to the config file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "ldsr = LDSR(modelPath=model_ckpt, yamlPath=config_file)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "config = OmegaConf.load(config_file)\n",
    "pl_sd = torch.load(model_ckpt, map_location=\"cpu\")\n",
    "sd = pl_sd[\"state_dict\"]\n",
    "model = ldsr.load_model_from_config()\n",
    "\n",
    "model['model'].load_state_dict(sd, strict=False)\n",
    "model['model'].to(device)\n",
    "model['model'].eval()\n",
    "\n",
    "# Load and prepare the input image\n",
    "input_image_path = \"example_lowres.png\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Normalize and prepare the image for processing\n",
    "input_image, w_pad, h_pad = LDSR.normalize_image(input_image)\n",
    "\n",
    "# Convert image to tensor\n",
    "input_tensor = torch.tensor(np.array(input_image)).float().div(255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Call the upscale function\n",
    "steps = \"100\"  # Example number of steps, adjust based on your needs\n",
    "pre_downscale = \"None\"\n",
    "post_downscale = \"None\"\n",
    "downsample_method = \"Lanczos\"\n",
    "\n",
    "# Initialize LDSRUpscale through the imported module\n",
    "upscaler = ldsrlib.LDSRUpscale()\n",
    "\n",
    "# Call the upscale method\n",
    "ldsr_output = upscaler.upscale(model, [input_tensor.to(device)], steps, pre_downscale, post_downscale, downsample_method)\n",
    "\n",
    "# Post-process the output\n",
    "output_image = ldsr_output[0].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "output_image = (output_image * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert numpy array back to image\n",
    "output_image_pil = Image.fromarray(output_image)\n",
    "\n",
    "# Save or display the output image\n",
    "output_image_pil.save(\"image_upscaled.png\")\n",
    "output_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logging\n",
    "import traceback\n",
    "def load_custom_node(module_path, ignore=set()):\n",
    "    module_name = os.path.basename(module_path)\n",
    "    if os.path.isfile(module_path):\n",
    "        sp = os.path.splitext(module_path)\n",
    "        module_name = sp[0]\n",
    "    try:\n",
    "        logging.debug(\"Trying to load custom node {}\".format(module_path))\n",
    "        if os.path.isfile(module_path):\n",
    "            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "            module_dir = os.path.split(module_path)[0]\n",
    "        else:\n",
    "            module_spec = importlib.util.spec_from_file_location(module_name, os.path.join(module_path, \"__init__.py\"))\n",
    "            module_dir = module_path\n",
    "\n",
    "        module = importlib.util.module_from_spec(module_spec)\n",
    "        sys.modules[module_name] = module\n",
    "        module_spec.loader.exec_module(module)\n",
    "    except Exception as e:\n",
    "        logging.warning(traceback.format_exc())\n",
    "        logging.warning(f\"Cannot import {module_path} module for custom nodes: {e}\")\n",
    "        return False\n",
    "\n",
    "load_custom_node('/Users/mihirpotnis/Downloads/ComfyUI-Flowty-LDSR-Master')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
